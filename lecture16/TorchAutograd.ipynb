{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differentiability**\n",
    "\n",
    "Before we talk about torch autograd it is important to remind ourselves what is meant by differentiability of a function of several variables.\n",
    "\n",
    "A function $f:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ is said to be differentiable at a point $x \\in \\mathbb{R}^d$ if there exists a linear function $D: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ with the property that\n",
    "\n",
    "$$\n",
    "\\lim_{h\\rightarrow 0} {f(x+h) - f(x) - D(h) \\over \\parallel h \\parallel}= 0\n",
    "$$\n",
    "\n",
    "We call the function $D$ the derviative $f$ at $x.$ \n",
    "\n",
    "When $f$ is differentiable at $x$ it is directionally differentiable, i.e. the limit\n",
    "\n",
    "$$\n",
    "\\lim_{t\\rightarrow 0}  {f(x+tv) - f(x) \\over t}\n",
    "$$\n",
    "\n",
    "exists, for any nonzero vector $v.$ When we take $v = e_i$ the unit vector $(0,0,\\ldots,0,1,0,\\ldots,0)$ then we obtain the partial derivative\n",
    "\n",
    "$$\n",
    "\\lim_{t\\rightarrow 0}  {f(x+te_i) - f(x) \\over t} = {\\partial f \\over \\partial x_i}, \n",
    "$$\n",
    "\n",
    "and the linear function $D$ is given by\n",
    "\n",
    "$$\n",
    "D(h) = \\sum_{i=1}^d {\\partial f \\over \\partial x_i} h_i\n",
    "$$\n",
    "\n",
    "Importantly, directional differentiability of a function does not guarantee its differentiability. In order to be differentiable at a point, a function that has directional derivative must \n",
    "\n",
    "A counterexample is give by the function\n",
    "\n",
    "$$\n",
    "f(x,y) = {yx^2 \\over x^2+y^2}.\n",
    "$$\n",
    "\n",
    "This function has partial derivative with respect to $x$ given by\n",
    "\n",
    "$$\n",
    "{\\partial f \\over \\partial x} = {2xy^3 \\over x^2+y^2}\n",
    "$$\n",
    "\n",
    "and this function is not continuous at $(0,0).$ Indeed, the limit along a linear path approaching $(0,0)$ depends on which path you take. \n",
    "\n",
    "Consider the path along a ray through the origin in the $\\theta$ direction. Take  $x = r \\cos(\\theta)$ and $y = r\\sin(\\theta)$ for fixed $\\theta$ letting $r \\rightarrow 0$ we obtain\n",
    "\n",
    "$$\n",
    "{2xy^3 \\over x^2+y^2}\\biggr\\rvert_{x = r\\cos(\\theta),y= r\\sin(\\theta)}\n",
    "= 2 \\cos(\\theta) \\sin^3(\\theta)\n",
    "$$\n",
    "\n",
    "which depends on $\\theta.$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Torch autograd**\n",
    "\n",
    "A key feature of Pytorch is autograd - the ability to store information about calculations on a tensor and generate gradients automatically in code.\n",
    "\n",
    "Gradients will be useful whenever we want to optimize some function, like a loss function when we fit a statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a tensor x, and tell pytorch to store gradient information when we create tensors that are functions of x. Ultimately, we compute the gradient of a scalar function of x.\n",
    "\n",
    "Let's start with a simple case of a dot product with a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 5., 7.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.tensor([0.,1.,2.,3.],requires_grad=True)\n",
    "y=torch.tensor([2.,3.,5.,7.])\n",
    "z=torch.dot(x,y)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And more complicated operations work. We just need to make sure that the operation is something that torch knows how to differentiate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create w as a function of x and compute the gradient of w with respect to x, which is a tensor of partial derivatives with respect to the components of x\n",
    "\n",
    "$ {\\partial ~ \\over \\partial x_j} w(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0646,  0.0498,  0.1184])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.tensor([1.,2.,3.],requires_grad=True)\n",
    "z=torch.sum(torch.sin(x))\n",
    "u=torch.log(1+z)\n",
    "w=torch.exp(-u)\n",
    "\n",
    "w.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try to do the same for u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "u.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3466, -1.3864, -9.3580])\n",
      "tensor([0.1911, 0.7645, 5.1603])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.tensor([1.,2.,3.],requires_grad=True)\n",
    "y=x[0]**1+x[1]**2+x[2]**3\n",
    "z=torch.sin(y)\n",
    "u=torch.log(1+z)\n",
    "w=torch.exp(-u)\n",
    "w.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "u.backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8069, -0.5974,  0.0394])\n",
      "tensor([ 0.1533, -0.3137, -0.9506])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.tensor([1.,2.,3.],requires_grad=True)\n",
    "y=torch.tensor([5.,7.,6.],requires_grad=True)\n",
    "z=torch.cos(x)*torch.sin(y)\n",
    "u=torch.sum(z)\n",
    "u.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "print(y.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
